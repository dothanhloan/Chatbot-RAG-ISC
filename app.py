import streamlit as st
import os
import sys

# ============================================
# C·∫§U H√åNH KEY (ƒêI·ªÄN ƒê·∫¶Y ƒê·ª¶ 2 KEY)
# ============================================
KEY_GOOGLE_MOI = "AIzaSy_D√°n_Key_Google_M·ªõi_V√†o_ƒê√¢y"
KEY_GROQ_CUA_BAN = ""
# ============================================

os.environ["GOOGLE_API_KEY"] = KEY_GOOGLE_MOI
GROQ_API_KEY = KEY_GROQ_CUA_BAN

# --- KH·∫ÆC PH·ª§C L·ªñI FONT TR√äN WINDOWS ---
sys.stdout.reconfigure(encoding='utf-8')
os.environ["PYTHONIOENCODING"] = "utf-8"

try:
    from langchain_community.document_loaders import Docx2txtLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    # Quay l·∫°i d√πng Google (V√¨ main.py c·ªßa b·∫°n ƒë√£ ch·∫°y ƒë∆∞·ª£c n√≥!)
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    from langchain_groq import ChatGroq
    from langchain_core.prompts import ChatPromptTemplate
except ImportError:
    st.error("‚ùå Thi·∫øu th∆∞ vi·ªán!")
    st.stop()

st.set_page_config(page_title="Chatbot AI T∆∞ V·∫•n", page_icon="ü§ñ")
st.title("ü§ñ Chatbot AI H·ªó Tr·ª£ T∆∞ V·∫•n")
st.write("---")

@st.cache_resource
def load_and_process_data():
    file_path = "data/input.docx"
    if not os.path.exists(file_path):
        return None
    
    loader = Docx2txtLoader(file_path)
    docs = loader.load()
    splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)
    
    # üõ†Ô∏è C·∫§U H√åNH ƒê·∫∂C BI·ªÜT ƒê·ªÇ KH√îNG B·ªä L·ªñI ASCII
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        google_api_key=KEY_GOOGLE_MOI,
        transport="rest",       # B·∫Øt bu·ªôc d√πng REST
        client_options={"api_endpoint": "generativelanguage.googleapis.com"}
    )
    
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore

with st.spinner("‚è≥ ƒêang k·∫øt n·ªëi Google AI..."):
    try:
        vectorstore = load_and_process_data()
    except Exception as e:
        st.error(f"‚ùå L·ªói Google: {e}")
        st.stop()

if vectorstore is None:
    st.error("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file 'data/input.docx'")
    st.stop()

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

try:
    llm = ChatGroq(temperature=0, model_name="llama-3.3-70b-versatile", api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"‚ùå L·ªói Groq: {e}")
    st.stop()

# --- CHAT LOOP ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("B·∫°n c·∫ßn h·ªèi g√¨?"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    try:
        relevant_docs = retriever.invoke(prompt)
        context = "\n\n".join([d.page_content for d in relevant_docs])
        
        # D√πng Prompt ti·∫øng Anh ƒë·ªÉ Groq hi·ªÉu t·ªët h∆°n, nh∆∞ng y√™u c·∫ßu tr·∫£ l·ªùi ti·∫øng Vi·ªát
        sys_prompt = ChatPromptTemplate.from_template(
            "Context: {context}\n\nQuestion: {question}\n\nAnswer in Vietnamese:"
        )
        
        chain = sys_prompt | llm
        response = chain.invoke({"context": context, "question": prompt})
        
        with st.chat_message("assistant"):
            st.markdown(response.content)
        st.session_state.messages.append({"role": "assistant", "content": response.content})

    except Exception as e:
        st.error(f"L·ªói: {e}")